#!/bin/bash

#PBS -A UCUB0166  
#PBS -N scaling
#PBS -j oe
#PBS -q develop 
#PBS -l walltime=0:10:00
#PBS -l select=1:ncpus=64:mpiprocs=1:ngpus=1:mem=84GB
#PBS -l gpu_type=a100

# Use moar processes for precompilation to speed things up
export JULIA_NUM_PRECOMPILE_TASKS=64
export JULIA_NUM_THREADS=64

# Load critical modules
module --force purge
module load ncarenv/23.09 nvhpc/24.7 cuda/12.2.1 cray-mpich/8.1.29

module list

# Utter mystical incantations to perform various miracles
export MPICH_GPU_SUPPORT_ENABLED=1
export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1
export JULIA_MPI_HAS_CUDA=true
export PALS_TRANSFER=false
export JULIA_CUDA_MEMORY_POOL=none

# Write down a script that binds MPI processes to GPUs (taken from Derecho documentation)
cat > launch.sh << EoF_s
#! /bin/bash

export MPICH_GPU_SUPPORT_ENABLED=1
export MPICH_GPU_MANAGED_MEMORY_SUPPORT_ENABLED=1
export LOCAL_RANK=\${PMI_LOCAL_RANK}
export GLOBAL_RANK=\${PMI_RANK}
export CUDA_VISIBLE_DEVICES=\$(expr \${LOCAL_RANK} % 1)

echo "Global Rank \${GLOBAL_RANK} / Local Rank \${LOCAL_RANK} / CUDA_VISIBLE_DEVICES=\${CUDA_VISIBLE_DEVICES} / \$(hostname)"

exec \$*

EoF_s

chmod +x launch.sh

# Now to make our julia environment work:
# 1. Instantiate (we only need to do this once, but this also may be the first time you are running this code)
julia --project -e 'using Pkg; Pkg.instantiate()'
# 2. Add some packages to the environment that we need to use
julia --project -e 'using Pkg; Pkg.add("MPI"); Pkg.add("MPIPreferences"); Pkg.add("CUDA"); Pkg.add("Oceananigans"); Pkg.add("Printf")'
# 3. Tell MPI that we would like to use the system binary we loaded with module load cray-mpich
julia --project -e 'using MPIPreferences; MPIPreferences.use_system_binary(vendor="cray")'
# 4. Build MPI and CUDA in advance for yucks
julia --project -e 'using MPI; using CUDA; CUDA.precompile_runtime()'
# Finally, let's run this thing
mpiexec -n 1 ./launch.sh julia --project oceananigans.jl
