
Currently Loaded Modules:
  1) ncarenv/23.09 (S)   4) craype/2.7.31          7) cray-mpich/8.1.29
  2) nvhpc/24.7          5) ncarcompilers/1.0.0
  3) cuda/12.2.1         6) gcc-toolchain/13.2.0

  Where:
   S:  Module is Sticky, requires --force to unload or purge

 

   Resolving package versions...
  No Changes to `/glade/work/apauls/oceananigans_scaling/Project.toml`
  No Changes to `/glade/work/apauls/oceananigans_scaling/Manifest.toml`
   Resolving package versions...
  No Changes to `/glade/work/apauls/oceananigans_scaling/Project.toml`
  No Changes to `/glade/work/apauls/oceananigans_scaling/Manifest.toml`
   Resolving package versions...
  No Changes to `/glade/work/apauls/oceananigans_scaling/Project.toml`
  No Changes to `/glade/work/apauls/oceananigans_scaling/Manifest.toml`
   Resolving package versions...
  No Changes to `/glade/work/apauls/oceananigans_scaling/Project.toml`
  No Changes to `/glade/work/apauls/oceananigans_scaling/Manifest.toml`
   Resolving package versions...
  No Changes to `/glade/work/apauls/oceananigans_scaling/Project.toml`
  No Changes to `/glade/work/apauls/oceananigans_scaling/Manifest.toml`
┌ Info: MPI implementation identified
│   libmpi = "libmpi_nvidia.so"
│   version_string = "MPI VERSION    : CRAY MPICH version 8.1.29.34 (ANL base 3.4a2)\nMPI BUILD INFO : Tue Feb 20 20:43 2024 (git hash d8ab47f)\n"
│   impl = "CrayMPICH"
│   version = v"8.1.29"
└   abi = "MPICH"
┌ Info: MPIPreferences unchanged
│   binary = "system"
│   libmpi = "libmpi_nvidia.so"
│   abi = "MPICH"
│   mpiexec = "mpiexec"
│   preloads = 1-element Vector{String}: …
└   preloads_env_switch = "MPICH_GPU_SUPPORT_ENABLED"
┌ Warning: CUDA runtime library `libcudart.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcudart.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcublasLt.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcublasLt.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libnvJitLink.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
Global Rank 0 / Local Rank 0 / CUDA_VISIBLE_DEVICES=0 / deg0027
┌ Warning: CUDA runtime library `libcublasLt.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcublasLt.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libnvJitLink.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libnvJitLink.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
┌ Warning: CUDA runtime library `libcusparse.so.12` was loaded from a system path, `/glade/u/apps/common/23.08/spack/opt/spack/cuda/12.2.1/lib64/libcusparse.so.12`.
│ This may cause errors.
│ 
│ If you're running under a profiler, this situation is expected. Otherwise,
│ ensure that your library path environment variable (e.g., `PATH` on Windows
│ or `LD_LIBRARY_PATH` on Linux) does not include CUDA library paths.
│ 
│ In any other case, please file an issue.
└ @ CUDA /glade/work/apauls/.julia/packages/CUDA/oymHm/src/initialization.jl:218
[ Info: Oceananigans will use 64 threads
[ Info: MPI has not been initialized, so we are calling MPI.Init().
Hello from process 0 out of 1
[ Info: The grid on rank 0:
┌ Info: 64×64×64 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on Distributed{GPU{CUDA.CUDAKernels.CUDABackend}} with 3×3×3 halo
│ ├── Periodic x ∈ [0.0, 1.0) regularly spaced with Δx=0.015625
│ ├── Periodic y ∈ [0.0, 1.0) regularly spaced with Δy=0.015625
└ └── Bounded  z ∈ [0.0, 1.0] regularly spaced with Δz=0.015625
[ Info: c on rank 0:
c = 64×64×64 Field{Center, Center, Center} on RectilinearGrid on Distributed{GPU{CUDA.CUDAKernels.CUDABackend}}
├── grid: 64×64×64 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on Distributed{GPU{CUDA.CUDAKernels.CUDABackend}} with 3×3×3 halo
├── boundary conditions: FieldBoundaryConditions
│   └── west: Periodic, east: Periodic, south: Periodic, north: Periodic, bottom: ZeroFlux, top: ZeroFlux, immersed: ZeroFlux
└── data: 70×70×70 OffsetArray(::CUDA.CuArray{Float64, 3, CUDA.DeviceMemory}, -2:67, -2:67, -2:67) with eltype Float64 with indices -2:67×-2:67×-2:67
    └── max=0.954031, min=2.27374e-13, mean=0.041659
[ Info: u on rank 0:
u = 64×64×64 Field{Face, Center, Center} on RectilinearGrid on Distributed{GPU{CUDA.CUDAKernels.CUDABackend}}
├── grid: 64×64×64 RectilinearGrid{Float64, Periodic, Periodic, Bounded} on Distributed{GPU{CUDA.CUDAKernels.CUDABackend}} with 3×3×3 halo
├── boundary conditions: FieldBoundaryConditions
│   └── west: Periodic, east: Periodic, south: Periodic, north: Periodic, bottom: ZeroFlux, top: ZeroFlux, immersed: ZeroFlux
└── data: 70×70×70 OffsetArray(::CUDA.CuArray{Float64, 3, CUDA.DeviceMemory}, -2:67, -2:67, -2:67) with eltype Float64 with indices -2:67×-2:67×-2:67
    └── max=0.946519, min=1.13687e-13, mean=0.0410132
